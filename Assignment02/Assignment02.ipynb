{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02\n",
    "\n",
    "```\n",
    "Build a binary classifier for human versus horse based on logistic regression using the dataset that consists of human and horse images\n",
    "```\n",
    "\n",
    "## Binary classification based on logistic regression\n",
    "\n",
    "$(x_i, y_i)$ denotes a pair of a training example and $i = 1, 2, \\cdots, n$\n",
    "\n",
    "$\\hat{y}_i = \\sigma(z_i)$ where $z_i = w^T x_i + b$ and $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    "\n",
    "The loss function is defined by $\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n f_i(w, b)$\n",
    "\n",
    "$f_i(w, b) = - y_i \\log \\hat{y}_i - (1 - y_i) \\log (1 - \\hat{y}_i) $\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- The dataset consists of human images and horse images for the training and the validation\n",
    "- The classifier should be trained using the training set\n",
    "- The classifier should be tested using the validation set\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- Write codes in python programming\n",
    "- Use ```jupyter notebook``` for the programming environment\n",
    "- You have to write your own implementation for the followings:\n",
    "    - compute the loss\n",
    "    - compute the accuracy\n",
    "    - compute the gradient of the model parameters with respect to the loss\n",
    "    - update the model parameters\n",
    "    - plot the results\n",
    "\n",
    "## Optimization\n",
    "\n",
    "- Apply the gradient descent algorithm with an appropriate learning rate\n",
    "- Apply the number of iterations that lead to the convergence of the algorith\n",
    "- Use the vectorization scheme in the computation of gradients and the update of the model parameters\n",
    "\n",
    "## git commit\n",
    "\n",
    "- Apply a number of ```git commit``` at intermediate development steps with their descriptive comments \n",
    "\n",
    "## Output\n",
    "\n",
    "- Plot the training loss at every iteration (x-axis: iteration, y-axis: loss)\n",
    "- Plot the validation loss at every iteration (x-axis: iteration, y-axis: loss)\n",
    "- Plot the training accuracy at every iteration (x-axis: iteration, y-axis: accuracy)\n",
    "- Plot the validation accuracy at every iteration (x-axis: iteration, y-axis: accuracy)\n",
    "- Present the table for the final accuracy and loss with training and validation datasets as below:\n",
    "\n",
    "| dataset    | loss       | accuracy   | \n",
    "|:----------:|:----------:|:----------:|\n",
    "| training   |            |            |\n",
    "| validation |            |            |\n",
    "\n",
    "## Submission\n",
    "\n",
    "- A PDF file exported from jupyter notebook for codes, results and comments [example: 20191234_02.pdf]\n",
    "- A PDF file exported from the github website for the history of git commit [example: 20191234_02_git.pdf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torchvision.transforms as transforms\\nimport torchvision\\nimport os\\n\\nNUM_EPOCH = 1\\n\\ntransform = transforms.Compose([#transforms.Resize((256,256)),  \\n                                transforms.Grayscale(),\\t\\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\\n                                transforms.ToTensor(),])\\n\\n\\n#train_data_path = 'relative path of training data set'\\ntrain_data_path = './horse-or-human/train'\\ntrainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\\n# change the valuse of batch_size, num_workers for your program\\n# if shuffle=True, the data reshuffled at every epoch \\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=3, shuffle=False, num_workers=1)  \\n\\n\\nvalidation_data_path = './horse-or-human/validation'\\nvalset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\\n# change the valuse of batch_size, num_workers for your program\\nvalloader = torch.utils.data.DataLoader(valset, batch_size=3, shuffle=False, num_workers=1)  \\n\\n\\nfor epoch in range(NUM_EPOCH):\\n    # load training images of the batch size for every iteration\\n    for i, data in enumerate(trainloader):\\n\\n        # inputs is the image\\n        # labels is the class of the image\\n        inputs, labels = data\\n\\n        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\\n        print(inputs.shape)\\n\\n        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\\n        print(labels)  \\n\\n\\n\\n\\n\\n    # load validation images of the batch size for every iteration\\n    for i, data in enumerate(valloader):\\n        \\n        # inputs is the image\\n        # labels is the class of the image\\n        inputs, labels = data\\n\\n        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\\n        print(inputs.shape)\\n\\n        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\\n        print(labels)    \\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "NUM_EPOCH = 1\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = './horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=3, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "validation_data_path = './horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=3, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        print(labels)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        print(labels)    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "training_data_path = './horse-or-human/train'\n",
    "validation_data_path = './horse-or-human/validation'\n",
    "\n",
    "training_data_list_horses = glob(os.path.join(training_data_path, 'horses/*'))\n",
    "training_data_list_humans = glob(os.path.join(training_data_path, 'humans/*'))\n",
    "\n",
    "validation_data_list_horses = glob(os.path.join(validation_data_path, 'horses/*'))\n",
    "validation_data_list_humans = glob(os.path.join(validation_data_path, 'humans/*'))\n",
    "\n",
    "# 0 for horses\n",
    "# 1 for humans\n",
    "\n",
    "width = 100\n",
    "height = 100\n",
    "\n",
    "number_of_training_data_horses = len(training_data_horses)\n",
    "number_of_training_data_humans = len(training_data_humans)\n",
    "\n",
    "number_of_validation_data_horses = len(validation_data_horses)\n",
    "number_of_validation_data_humans = len(validation_data_humans)\n",
    "\n",
    "number_of_training_data = number_of_training_data_horses + number_of_training_data_humans\n",
    "number_of_validation_data = number_of_validation_data_horses + number_of_validation_data_humans\n",
    "\n",
    "training_data_horses = np.zeros((len(training_data_list_horses), width, height))\n",
    "training_data_humans = np.zeros((len(training_data_list_humans), width, height))\n",
    "\n",
    "validation_data_horses = np.zeros((len(validation_data_list_horses), width, height))\n",
    "validation_data_humans = np.zeros((len(validation_data_list_humans), width, height))\n",
    "\n",
    "# index x width x height\n",
    "for i, fname in enumerate(training_data_list_horses):\n",
    "    tmp = Image.open(fname)\n",
    "    training_data_horses[i,:,:] = np.array(tmp)\n",
    "\n",
    "for i, fname in enumerate(training_data_list_humans):\n",
    "    tmp = Image.open(fname)\n",
    "    training_data_humans[i,:,:] = np.array(tmp)\n",
    "\n",
    "for i, fname in enumerate(validation_data_list_horses):\n",
    "    tmp = Image.open(fname)\n",
    "    validation_data_horses[i,:,:] = np.array(tmp)\n",
    "    \n",
    "for i, fname in enumerate(validation_data_list_humans):\n",
    "    tmp = Image.open(fname)\n",
    "    validation_data_humans[i,:,:] = np.array(tmp)\n",
    "\n",
    "training_data = np.vstack((training_data_horses, training_data_humans))\n",
    "validation_data = np.vstack((validation_data_horses, validation_data_humans))\n",
    "\n",
    "training_data_label = np.ones((number_of_training_data))\n",
    "validation_data_label = np.ones((number_of_validation_data))\n",
    "for i in range(len(training_data_list_horses)):\n",
    "    training_data_label[i] = 0\n",
    "for i in range(len(validation_data_list_horses)):\n",
    "    validation_data_label[i] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to index x data\n",
    "training_data = training_data.reshape(number_of_training_data, width*height)\n",
    "validation_data = validation_data.reshape(number_of_validation_data, width*height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    result = 1+np.exp(-x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_hat):\n",
    "    result = -y*np.log(y_hat) - (1-y)*np.log(1-y_hat)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, x, h=1e-5):\n",
    "    result = (f(x+h)-f(x)) / h\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "# \"\"\" 함수 f의 i번째 편도함수가 v에서 가지는 값 \"\"\"\n",
    "\n",
    "    w = [v_j + (h if j == i else 0) # h를 v의 i번째 변수에만 더해주자.\n",
    "        for j, v_j in enumerate(v)] # 즉 i 번째 변수만 변화할 경우\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "# 출처: https://hamait.tistory.com/747 [HAMA 블로그]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
