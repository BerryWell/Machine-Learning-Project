{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01\n",
    "\n",
    "## General Instruction\n",
    "\n",
    "#### Jupyter Notebook\n",
    "\n",
    "```console\n",
    "- Write programming codes in python\n",
    "- Use Jupyter Notebook for writing codes\n",
    "- Include comments and intermediate results in addition to the codes\n",
    "- Export the Jupyter Notebook file in PDF format\n",
    "- Turn in the PDF file at Google Classroom (late submission is not allowed)\n",
    "```\n",
    "\n",
    "#### History of git commits\n",
    "\n",
    "```console\n",
    "- Create a private repository at github \n",
    "- Commit intermediate status of working file at given steps\n",
    "- Export the history of commits in PDF format\n",
    "- Turn in the PDF file at Google Classroom (late submission is not allowed)\n",
    "```\n",
    "\n",
    "## Binary Classification based on Logistic Regression\n",
    "\n",
    "> - $(x_i, y_i)$ denotes a pair of a training example and $i = 1, 2, \\cdots, n$\n",
    "> - $\\hat{y}_i = \\sigma(z_i)$ where $z_i = w^T x_i + b$ and $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    "> - The loss function is defined by $\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n f_i(w, b)$\n",
    "> - $f_i(w, b) = - y_i \\log \\hat{y}_i - (1 - y_i) \\log (1 - \\hat{y}_i) $\n",
    "\n",
    "### 1. Plot two clusters of points for training dateset\n",
    "\n",
    "- Generate two sets of separable random point clusters in $\\mathbb{R}^2$\n",
    "- Let $\\{ x_i \\}_{i=1}^n$ be a set of points and $\\{ y_i \\}_{i=1}^n$ be their corresponding labels\n",
    "- Plot the point clusters in the training dataset using different colors depending on their labels\n",
    "\n",
    "### 2. Plot two clusters of points for testing dataset\n",
    "\n",
    "- Generate two sets of separable random point clusters in $\\mathbb{R}^2$ for a testing dataset using the same centroid and the standard deviation of random generator as the training dataset\n",
    "- Plot the point clusters in the testing dataset using different colors depending on their labels (different colors from the training dataset)\n",
    "\n",
    "### 3. git commit\n",
    "\n",
    "```console\n",
    "$ git commit -a -m \"Plot the training and testing datasets\"\n",
    "$ git push -u origin master\n",
    "```\n",
    "\n",
    "### 4. Plot the learning curves\n",
    "\n",
    "- Apply the gradient descent algorithm\n",
    "- Plot the training loss at every iteration\n",
    "- Plot the testing loss at every iteration\n",
    "- Plot the training accuracy at every iteration\n",
    "- Plot the testing accuracy at every iteration\n",
    "\n",
    "### 5. git commit\n",
    "\n",
    "```console\n",
    "$ git commit -a -m \"Plot the learning curves\"\n",
    "$ git push -u origin master\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "4) 로지스틱 회귀(Logistic Regression) - 이진 분류(https://wikidocs.net/22881)\n",
    "\n",
    "How To Implement Logistic Regression From Scratch in Python(https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/)\n",
    "\n",
    "gradient descent 구현(https://hwiyong.tistory.com/71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(numOfPoints):\n",
    "    values = np.random.rand(numOfPoints)\n",
    "    labels = np.random.randint(2, size=numOfPoints)\n",
    "    data = np.vstack((values, labels))\n",
    "    data = data.transpose()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "[[0.92644186 1.        ]\n",
      " [0.41899755 0.        ]\n",
      " [0.52553393 0.        ]\n",
      " [0.19117304 1.        ]\n",
      " [0.98541544 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = generateData(5)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss():\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    z = w*x + b\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcfe94b3cf8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "old_value = 10\n",
    "derivative = [old_value]\n",
    "# basic example : x^2\n",
    "y = [old_value ** 2]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(1, 1000):\n",
    "    # update\n",
    "    new_value = old_value - learning_rate * old_value\n",
    "    \n",
    "    # for plotting\n",
    "    y.append(new_value ** 2)\n",
    "    derivative.append(new_value)\n",
    "    \n",
    "    old_value = new_value\n",
    "    \n",
    "plt.plot(derivative, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nam/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/nam/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 5.9131 - binary_accuracy: 0.3077\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2085 - binary_accuracy: 0.9231\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2051 - binary_accuracy: 0.9231\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2028 - binary_accuracy: 0.9231\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2009 - binary_accuracy: 0.9231\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1991 - binary_accuracy: 0.9231\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1975 - binary_accuracy: 0.9231\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1959 - binary_accuracy: 0.9231\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1943 - binary_accuracy: 0.9231\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1927 - binary_accuracy: 0.9231\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1912 - binary_accuracy: 0.9231\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1897 - binary_accuracy: 0.9231\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1882 - binary_accuracy: 0.9231\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1868 - binary_accuracy: 0.9231\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1854 - binary_accuracy: 0.9231\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1840 - binary_accuracy: 0.9231\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1826 - binary_accuracy: 0.9231\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1812 - binary_accuracy: 0.9231\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1799 - binary_accuracy: 0.9231\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1786 - binary_accuracy: 0.9231\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1773 - binary_accuracy: 0.9231\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1761 - binary_accuracy: 0.9231\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1748 - binary_accuracy: 0.9231\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1736 - binary_accuracy: 0.9231\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1724 - binary_accuracy: 0.9231\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1712 - binary_accuracy: 0.9231- ETA: 0s - loss: 0.2561 - binary_accuracy: 0.8750   \n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1700 - binary_accuracy: 0.9231\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1689 - binary_accuracy: 0.9231\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1677 - binary_accuracy: 0.9231\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1666 - binary_accuracy: 0.9231\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1655 - binary_accuracy: 0.9231\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1644 - binary_accuracy: 0.9231\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1634 - binary_accuracy: 0.9231\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1623 - binary_accuracy: 0.9231\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1613 - binary_accuracy: 0.9231\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1603 - binary_accuracy: 0.9231\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1593 - binary_accuracy: 0.9231\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1583 - binary_accuracy: 0.9231\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1573 - binary_accuracy: 0.9231\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1563 - binary_accuracy: 0.9231\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1554 - binary_accuracy: 0.9231\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1545 - binary_accuracy: 0.9231\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1536 - binary_accuracy: 0.9231\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1527 - binary_accuracy: 0.9231\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1518 - binary_accuracy: 0.9231\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1509 - binary_accuracy: 0.9231\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1500 - binary_accuracy: 0.9231\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1492 - binary_accuracy: 0.9231\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1483 - binary_accuracy: 0.9231\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1475 - binary_accuracy: 0.9231\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1467 - binary_accuracy: 0.9231\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1459 - binary_accuracy: 0.9231\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1451 - binary_accuracy: 0.9231\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1443 - binary_accuracy: 0.9231\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1435 - binary_accuracy: 0.9231\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1428 - binary_accuracy: 0.9231\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1420 - binary_accuracy: 0.9231\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1413 - binary_accuracy: 0.9231\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1406 - binary_accuracy: 0.9231\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1398 - binary_accuracy: 0.9231\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1391 - binary_accuracy: 0.9231\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1384 - binary_accuracy: 0.9231\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1377 - binary_accuracy: 0.9231\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1371 - binary_accuracy: 0.9231\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1364 - binary_accuracy: 0.9231\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1357 - binary_accuracy: 0.9231\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1351 - binary_accuracy: 0.9231\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1344 - binary_accuracy: 0.9231\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1338 - binary_accuracy: 0.9231\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1331 - binary_accuracy: 0.9231\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1325 - binary_accuracy: 0.9231\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1319 - binary_accuracy: 0.9231\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1313 - binary_accuracy: 0.9231\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1307 - binary_accuracy: 0.9231\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1301 - binary_accuracy: 0.9231\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1295 - binary_accuracy: 0.9231\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1289 - binary_accuracy: 0.9231\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1284 - binary_accuracy: 0.9231\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1278 - binary_accuracy: 0.9231\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1272 - binary_accuracy: 0.9231\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1267 - binary_accuracy: 0.9231\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1261 - binary_accuracy: 0.9231\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1256 - binary_accuracy: 0.9231\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1251 - binary_accuracy: 0.9231\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1246 - binary_accuracy: 0.9231\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1240 - binary_accuracy: 0.9231\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1235 - binary_accuracy: 0.9231\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1230 - binary_accuracy: 0.9231\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1225 - binary_accuracy: 0.9231\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1220 - binary_accuracy: 0.9231\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1215 - binary_accuracy: 0.9231\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1210 - binary_accuracy: 0.9231\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1206 - binary_accuracy: 0.9231\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1201 - binary_accuracy: 0.9231\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1196 - binary_accuracy: 0.9231\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1192 - binary_accuracy: 0.9231\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1187 - binary_accuracy: 0.9231\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1182 - binary_accuracy: 0.9231\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1178 - binary_accuracy: 0.9231\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1173 - binary_accuracy: 0.9231\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1169 - binary_accuracy: 0.9231\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1165 - binary_accuracy: 0.9231\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1160 - binary_accuracy: 0.9231\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1156 - binary_accuracy: 0.9231\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1152 - binary_accuracy: 0.9231\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1148 - binary_accuracy: 0.9231\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1144 - binary_accuracy: 0.9231\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1140 - binary_accuracy: 0.9231\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1136 - binary_accuracy: 0.9231\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1132 - binary_accuracy: 0.9231\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1128 - binary_accuracy: 0.9231\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1124 - binary_accuracy: 0.9231\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1120 - binary_accuracy: 0.9231\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1116 - binary_accuracy: 0.9231\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1112 - binary_accuracy: 0.9231\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1108 - binary_accuracy: 0.9231\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1105 - binary_accuracy: 0.9231\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1101 - binary_accuracy: 0.9231\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1097 - binary_accuracy: 0.9231\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1094 - binary_accuracy: 0.9231\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1090 - binary_accuracy: 0.9231\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1087 - binary_accuracy: 0.9231\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1083 - binary_accuracy: 0.9231\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1080 - binary_accuracy: 0.9231\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1076 - binary_accuracy: 0.9231\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1073 - binary_accuracy: 0.9231\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1069 - binary_accuracy: 0.9231\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1066 - binary_accuracy: 0.9231\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1063 - binary_accuracy: 0.9231\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1059 - binary_accuracy: 0.9231\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1056 - binary_accuracy: 0.9231\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1053 - binary_accuracy: 0.9231\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1050 - binary_accuracy: 0.9231\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7007e-07 - binary_accuracy: 1.000 - 0s 3ms/step - loss: 0.1046 - binary_accuracy: 0.9231\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1043 - binary_accuracy: 0.9231\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1040 - binary_accuracy: 0.9231\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1037 - binary_accuracy: 0.9231\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1034 - binary_accuracy: 0.9231\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1031 - binary_accuracy: 0.9231\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1028 - binary_accuracy: 0.9231\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1025 - binary_accuracy: 0.9231\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1022 - binary_accuracy: 0.9231\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1019 - binary_accuracy: 0.9231\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1016 - binary_accuracy: 0.9231\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1013 - binary_accuracy: 0.9231\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1010 - binary_accuracy: 0.9231\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1008 - binary_accuracy: 0.9231\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1005 - binary_accuracy: 0.9231\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1002 - binary_accuracy: 0.9231\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0999 - binary_accuracy: 0.9231\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0996 - binary_accuracy: 0.9231\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0994 - binary_accuracy: 0.9231\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0991 - binary_accuracy: 0.9231\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0988 - binary_accuracy: 0.9231\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0986 - binary_accuracy: 0.9231\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0983 - binary_accuracy: 0.9231\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0980 - binary_accuracy: 0.9231\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0978 - binary_accuracy: 0.9231\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0975 - binary_accuracy: 0.9231\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0973 - binary_accuracy: 0.9231\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0970 - binary_accuracy: 0.9231\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0968 - binary_accuracy: 0.9231\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0965 - binary_accuracy: 0.9231\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0963 - binary_accuracy: 0.9231\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0960 - binary_accuracy: 0.9231\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0958 - binary_accuracy: 0.9231\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0955 - binary_accuracy: 0.9231\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0953 - binary_accuracy: 0.9231\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0950 - binary_accuracy: 0.9231\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0948 - binary_accuracy: 0.9231\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0946 - binary_accuracy: 0.9231\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0943 - binary_accuracy: 0.9231\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0941 - binary_accuracy: 0.9231\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0939 - binary_accuracy: 0.9231\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0937 - binary_accuracy: 0.9231\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0934 - binary_accuracy: 0.9231\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0932 - binary_accuracy: 0.9231\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0930 - binary_accuracy: 0.9231\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0928 - binary_accuracy: 0.9231\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0925 - binary_accuracy: 0.9231\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0923 - binary_accuracy: 0.9231\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0921 - binary_accuracy: 0.9231\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0919 - binary_accuracy: 0.9231\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0917 - binary_accuracy: 0.9231\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0915 - binary_accuracy: 0.9231\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0912 - binary_accuracy: 0.9231\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0910 - binary_accuracy: 0.9231\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0908 - binary_accuracy: 0.9231\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0906 - binary_accuracy: 0.9231\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0904 - binary_accuracy: 0.9231\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0902 - binary_accuracy: 0.9231\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0900 - binary_accuracy: 0.9231\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0898 - binary_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0896 - binary_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0894 - binary_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0892 - binary_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0890 - binary_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0888 - binary_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0886 - binary_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0884 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0a6711240>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential # 케라스의 Sequential()을 임포트\n",
    "from keras.layers import Dense # 케라스의 Dense()를 임포트\n",
    "from keras import optimizers # 케라스의 옵티마이저를 임포트\n",
    "import numpy as np # Numpy를 임포트\n",
    "\n",
    "X=np.array([-50, -40, -30, -20, -10, -5, 0, 5, 10, 20, 30, 40, 50])\n",
    "y=np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) #숫자 10부터 1\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd ,loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
    "# 옵티마이저는 경사하강법의 일종인 확률적 경사 하강법 sgd를 사용합니다.\n",
    "# 손실 함수(Loss function)는 binary_crossentropy(이진 크로스 엔트로피)를 사용합니다.\n",
    "model.fit(X,y, batch_size=1, epochs=200, shuffle=False)\n",
    "# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 200번 시도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.211413  ]\n",
      " [0.2698232 ]\n",
      " [0.3374652 ]\n",
      " [0.41248512]\n",
      " [0.45183808]]\n",
      "[[0.86905885]\n",
      " [0.9939507 ]\n",
      " [0.9997542 ]\n",
      " [0.9999901 ]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([1, 2, 3, 4, 4.5]))\n",
    "print(model.predict([11, 21, 31, 41, 500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
